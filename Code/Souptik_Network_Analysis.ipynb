{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chicago Community Area Network Analysis\n",
    "## Souptik's Independent Network and Metrics Analysis\n",
    "\n",
    "This notebook:\n",
    "1. Loads and aggregates data to 9 Community Areas (CAs)\n",
    "2. Constructs multiple network graph versions with different thresholds\n",
    "3. Computes centrality measures (degree, eigenvector, closeness)\n",
    "4. Runs community detection (Louvain + hierarchical clustering)\n",
    "5. Calculates clustering coefficients using block-group level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport networkx as nx\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom community import community_louvain\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set plotting style with WHITE background\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('husl')\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.facecolor'] = 'white'\n\nprint(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Aggregation to 9 Community Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the aggregated data\n",
    "# Adjust path as needed\n",
    "df = pd.read_csv('data_agg/agg_data.csv')\n",
    "\n",
    "print(f\"Raw data shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 9 Community Areas\n",
    "target_cas = [\n",
    "    'Englewood',\n",
    "    'West_Englewood', \n",
    "    'Irving_Park',\n",
    "    'JeffersonPark',\n",
    "    'Lakeview',\n",
    "    'LincolnPark',\n",
    "    'NearNorthSide',\n",
    "    'Portage_Park',\n",
    "    'South_Lawndale'\n",
    "]\n",
    "\n",
    "print(f\"Target Community Areas: {target_cas}\")\n",
    "print(f\"Number of CAs: {len(target_cas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to Community Area level\n",
    "# This assumes the data has a community area identifier column\n",
    "# Adjust column names as needed based on actual data structure\n",
    "\n",
    "# Identify numeric columns for aggregation\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Identify the CA identifier column (adjust as needed)\n",
    "ca_col = None\n",
    "for col in df.columns:\n",
    "    if 'community' in col.lower() or 'area' in col.lower() or 'ca' in col.lower():\n",
    "        ca_col = col\n",
    "        break\n",
    "\n",
    "if ca_col is None:\n",
    "    # If no CA column found, check if data is already at CA level\n",
    "    print(\"Checking data structure...\")\n",
    "    print(df.columns.tolist())\n",
    "else:\n",
    "    print(f\"Community Area column identified: {ca_col}\")\n",
    "\n",
    "# Alternative: aggregate by parsing from the raw ONSA data\n",
    "# This will load individual CA files and aggregate them\n",
    "ca_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for each Community Area from individual files\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('data_agg/ONSA_Data')\n",
    "\n",
    "# Demographic/socioeconomic variables to extract\n",
    "variables = [\n",
    "    'total_population',\n",
    "    'median_income',\n",
    "    'poverty',\n",
    "    'unemployment',\n",
    "    'bachelors',\n",
    "    'graduate',\n",
    "    'white',\n",
    "    'black',\n",
    "    'hispanic',\n",
    "    'broadband',\n",
    "    'owner_occupied',\n",
    "    'renter_occupied',\n",
    "    'snap'\n",
    "]\n",
    "\n",
    "ca_aggregated = {}\n",
    "\n",
    "for ca in target_cas:\n",
    "    ca_dir = data_dir / ca\n",
    "    if ca_dir.exists():\n",
    "        ca_data = {}\n",
    "        \n",
    "        for var in variables:\n",
    "            # Find the file for this variable\n",
    "            pattern = f\"{ca}_{var}_2018_2023.csv\"\n",
    "            file_path = ca_dir / pattern\n",
    "            \n",
    "            if file_path.exists():\n",
    "                temp_df = pd.read_csv(file_path)\n",
    "                # Get most recent year (2023) or aggregate across years\n",
    "                # Assuming structure has year columns or rows\n",
    "                \n",
    "                # Strategy 1: Take mean across available years\n",
    "                numeric_data = temp_df.select_dtypes(include=[np.number])\n",
    "                if len(numeric_data.columns) > 0:\n",
    "                    ca_data[var] = numeric_data.mean().mean()  # Overall mean\n",
    "        \n",
    "        ca_aggregated[ca] = ca_data\n",
    "        print(f\"Loaded data for {ca}: {len(ca_data)} variables\")\n",
    "    else:\n",
    "        print(f\"Warning: Directory not found for {ca}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "ca_df = pd.DataFrame(ca_aggregated).T\n",
    "ca_df.index.name = 'Community_Area'\n",
    "ca_df = ca_df.reset_index()\n",
    "\n",
    "print(f\"\\nAggregated CA data shape: {ca_df.shape}\")\n",
    "ca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values and normalize data\n",
    "ca_df_clean = ca_df.copy()\n",
    "\n",
    "# Fill missing values with median\n",
    "numeric_cols = ca_df_clean.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    ca_df_clean[col].fillna(ca_df_clean[col].median(), inplace=True)\n",
    "\n",
    "# Standardize the features for similarity calculations\n",
    "scaler = StandardScaler()\n",
    "ca_features = ca_df_clean[numeric_cols].values\n",
    "ca_features_scaled = scaler.fit_transform(ca_features)\n",
    "ca_features_df = pd.DataFrame(ca_features_scaled, \n",
    "                               columns=numeric_cols,\n",
    "                               index=ca_df_clean['Community_Area'])\n",
    "\n",
    "print(\"Data normalized and ready for network construction\")\n",
    "print(f\"Feature matrix shape: {ca_features_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the aggregated CA-level data\n",
    "ca_df.to_csv('CA_Aggregated_Data.csv', index=False)\n",
    "ca_features_df.to_csv('CA_Normalized_Features.csv')\n",
    "print(\"Saved: CA_Aggregated_Data.csv and CA_Normalized_Features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Network Graph Construction (Multiple Versions)\n",
    "\n",
    "We'll create networks using:\n",
    "1. Cosine similarity with different thresholds\n",
    "2. Euclidean distance (inverted) with different thresholds\n",
    "3. Top-k nearest neighbors\n",
    "4. Correlation-based similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity matrices\n",
    "\n",
    "# 1. Cosine similarity\n",
    "cosine_sim = cosine_similarity(ca_features_scaled)\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim, \n",
    "                              index=ca_df_clean['Community_Area'],\n",
    "                              columns=ca_df_clean['Community_Area'])\n",
    "\n",
    "# 2. Euclidean distance (convert to similarity: smaller distance = higher similarity)\n",
    "euclidean_dist = euclidean_distances(ca_features_scaled)\n",
    "# Convert to similarity: similarity = 1 / (1 + distance)\n",
    "euclidean_sim = 1 / (1 + euclidean_dist)\n",
    "euclidean_sim_df = pd.DataFrame(euclidean_sim,\n",
    "                                 index=ca_df_clean['Community_Area'],\n",
    "                                 columns=ca_df_clean['Community_Area'])\n",
    "\n",
    "# 3. Correlation-based similarity\n",
    "correlation_sim = np.corrcoef(ca_features_scaled)\n",
    "correlation_sim_df = pd.DataFrame(correlation_sim,\n",
    "                                   index=ca_df_clean['Community_Area'],\n",
    "                                   columns=ca_df_clean['Community_Area'])\n",
    "\n",
    "print(\"Similarity matrices computed:\")\n",
    "print(f\"Cosine similarity range: [{cosine_sim.min():.3f}, {cosine_sim.max():.3f}]\")\n",
    "print(f\"Euclidean similarity range: [{euclidean_sim.min():.3f}, {euclidean_sim.max():.3f}]\")\n",
    "print(f\"Correlation similarity range: [{correlation_sim.min():.3f}, {correlation_sim.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create network from similarity matrix\n",
    "def create_network_from_similarity(sim_matrix, nodes, threshold=0.5, method='threshold'):\n",
    "    \"\"\"\n",
    "    Create network from similarity matrix\n",
    "    \n",
    "    Parameters:\n",
    "    - sim_matrix: similarity matrix (numpy array)\n",
    "    - nodes: list of node names\n",
    "    - threshold: similarity threshold for edge creation\n",
    "    - method: 'threshold' or 'top_k'\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    \n",
    "    n = len(nodes)\n",
    "    \n",
    "    if method == 'threshold':\n",
    "        # Add edge if similarity exceeds threshold\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if sim_matrix[i, j] >= threshold:\n",
    "                    G.add_edge(nodes[i], nodes[j], weight=sim_matrix[i, j])\n",
    "    \n",
    "    elif method == 'top_k':\n",
    "        # Add edges to top k most similar nodes for each node\n",
    "        k = int(threshold)  # In this case, threshold is actually k\n",
    "        for i in range(n):\n",
    "            # Get top k similar nodes (excluding self)\n",
    "            similarities = sim_matrix[i, :].copy()\n",
    "            similarities[i] = -np.inf  # Exclude self\n",
    "            top_k_indices = np.argsort(similarities)[-k:]\n",
    "            \n",
    "            for j in top_k_indices:\n",
    "                if i < j:  # Avoid duplicate edges\n",
    "                    G.add_edge(nodes[i], nodes[j], weight=sim_matrix[i, j])\n",
    "    \n",
    "    return G\n",
    "\n",
    "print(\"Network creation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple network versions\n",
    "networks = {}\n",
    "nodes = ca_df_clean['Community_Area'].tolist()\n",
    "\n",
    "# Version 1: Cosine similarity with threshold 0.7\n",
    "networks['cosine_0.7'] = create_network_from_similarity(\n",
    "    cosine_sim, nodes, threshold=0.7, method='threshold'\n",
    ")\n",
    "\n",
    "# Version 2: Cosine similarity with threshold 0.5\n",
    "networks['cosine_0.5'] = create_network_from_similarity(\n",
    "    cosine_sim, nodes, threshold=0.5, method='threshold'\n",
    ")\n",
    "\n",
    "# Version 3: Cosine similarity with threshold 0.3\n",
    "networks['cosine_0.3'] = create_network_from_similarity(\n",
    "    cosine_sim, nodes, threshold=0.3, method='threshold'\n",
    ")\n",
    "\n",
    "# Version 4: Euclidean similarity with threshold 0.6\n",
    "networks['euclidean_0.6'] = create_network_from_similarity(\n",
    "    euclidean_sim, nodes, threshold=0.6, method='threshold'\n",
    ")\n",
    "\n",
    "# Version 5: Top-3 nearest neighbors\n",
    "networks['top_3'] = create_network_from_similarity(\n",
    "    cosine_sim, nodes, threshold=3, method='top_k'\n",
    ")\n",
    "\n",
    "# Version 6: Top-4 nearest neighbors\n",
    "networks['top_4'] = create_network_from_similarity(\n",
    "    cosine_sim, nodes, threshold=4, method='top_k'\n",
    ")\n",
    "\n",
    "# Version 7: Correlation-based with threshold 0.5\n",
    "networks['correlation_0.5'] = create_network_from_similarity(\n",
    "    correlation_sim, nodes, threshold=0.5, method='threshold'\n",
    ")\n",
    "\n",
    "print(\"\\nNetwork Statistics:\")\n",
    "print(\"=\"*60)\n",
    "for name, G in networks.items():\n",
    "    print(f\"{name:20s} | Nodes: {G.number_of_nodes():2d} | Edges: {G.number_of_edges():2d} | Density: {nx.density(G):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize networks\nfig, axes = plt.subplots(3, 3, figsize=(18, 18))\nfig.patch.set_facecolor('white')\naxes = axes.flatten()\n\nfor idx, (name, G) in enumerate(networks.items()):\n    if idx < 9:\n        ax = axes[idx]\n        ax.set_facecolor('white')\n        pos = nx.spring_layout(G, seed=42, k=1.5)\n        \n        # Draw network with better colors for white background\n        nx.draw_networkx_nodes(G, pos, node_size=800, \n                               node_color='#3498db',  # Nice blue\n                               edgecolors='#2c3e50',  # Dark border\n                               linewidths=2,\n                               alpha=0.9, ax=ax)\n        nx.draw_networkx_edges(G, pos, width=2, alpha=0.4,\n                               edge_color='#7f8c8d', ax=ax)  # Gray edges\n        nx.draw_networkx_labels(G, pos, font_size=8,\n                                font_weight='bold',\n                                font_color='#2c3e50', ax=ax)\n        \n        ax.set_title(f\"{name}\\n({G.number_of_edges()} edges)\", \n                    fontsize=11, fontweight='bold', color='#2c3e50')\n        ax.axis('off')\n\n# Remove unused subplots\nfor idx in range(len(networks), 9):\n    axes[idx].axis('off')\n    axes[idx].set_facecolor('white')\n\nplt.tight_layout()\nplt.savefig('Network_Graphs_All_Versions.png', dpi=150, bbox_inches='tight', facecolor='white')\nplt.show()\n\nprint(\"Network visualizations saved: Network_Graphs_All_Versions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Centrality Measures\n",
    "\n",
    "Computing three key centrality measures for each network:\n",
    "- **Degree Centrality**: Number of connections\n",
    "- **Eigenvector Centrality**: Importance based on connections to important nodes\n",
    "- **Closeness Centrality**: Average distance to all other nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate centrality measures for all networks\n",
    "centrality_results = []\n",
    "\n",
    "for name, G in networks.items():\n",
    "    # Degree centrality\n",
    "    degree_cent = nx.degree_centrality(G)\n",
    "    \n",
    "    # Eigenvector centrality (with fallback for disconnected graphs)\n",
    "    try:\n",
    "        eigen_cent = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "    except:\n",
    "        eigen_cent = {node: 0 for node in G.nodes()}\n",
    "    \n",
    "    # Closeness centrality\n",
    "    try:\n",
    "        closeness_cent = nx.closeness_centrality(G)\n",
    "    except:\n",
    "        closeness_cent = {node: 0 for node in G.nodes()}\n",
    "    \n",
    "    # Combine into dataframe\n",
    "    for node in G.nodes():\n",
    "        centrality_results.append({\n",
    "            'Network': name,\n",
    "            'Community_Area': node,\n",
    "            'Degree_Centrality': degree_cent[node],\n",
    "            'Eigenvector_Centrality': eigen_cent[node],\n",
    "            'Closeness_Centrality': closeness_cent[node]\n",
    "        })\n",
    "\n",
    "centrality_df = pd.DataFrame(centrality_results)\n",
    "centrality_df.to_csv('Centrality_Measures_All_Networks.csv', index=False)\n",
    "\n",
    "print(\"Centrality measures computed and saved: Centrality_Measures_All_Networks.csv\")\n",
    "print(f\"\\nSample results:\")\n",
    "centrality_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for centrality measures\n",
    "centrality_summary = centrality_df.groupby('Network')[[\n",
    "    'Degree_Centrality', 'Eigenvector_Centrality', 'Closeness_Centrality'\n",
    "]].agg(['mean', 'std', 'min', 'max'])\n",
    "\n",
    "print(\"\\nCentrality Summary Statistics:\")\n",
    "print(centrality_summary)\n",
    "\n",
    "centrality_summary.to_csv('Centrality_Summary_Statistics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize centrality measures for selected network (e.g., cosine_0.5)\n",
    "selected_network = 'cosine_0.5'\n",
    "selected_data = centrality_df[centrality_df['Network'] == selected_network]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "measures = ['Degree_Centrality', 'Eigenvector_Centrality', 'Closeness_Centrality']\n",
    "titles = ['Degree Centrality', 'Eigenvector Centrality', 'Closeness Centrality']\n",
    "\n",
    "for ax, measure, title in zip(axes, measures, titles):\n",
    "    selected_data_sorted = selected_data.sort_values(measure, ascending=True)\n",
    "    ax.barh(selected_data_sorted['Community_Area'], selected_data_sorted[measure])\n",
    "    ax.set_xlabel(title)\n",
    "    ax.set_title(f\"{title}\\n(Network: {selected_network})\")\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'Centrality_Visualization_{selected_network}.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Centrality visualization saved for {selected_network}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Community Detection\n",
    "\n",
    "Running two community detection algorithms:\n",
    "1. **Louvain Algorithm**: Modularity-based community detection\n",
    "2. **Hierarchical Clustering**: Agglomerative clustering on similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Louvain community detection\nlouvain_results = []\n\nfor name, G in networks.items():\n    if G.number_of_edges() > 0:\n        # Create a copy with positive weights only\n        G_positive = G.copy()\n        \n        # Check for negative weights\n        has_negative = False\n        for u, v, data in G_positive.edges(data=True):\n            if 'weight' in data and data['weight'] < 0:\n                has_negative = True\n                break\n        \n        if has_negative:\n            # Shift all weights to be positive\n            weights = [data['weight'] for u, v, data in G_positive.edges(data=True) if 'weight' in data]\n            min_weight = min(weights)\n            \n            for u, v, data in G_positive.edges(data=True):\n                if 'weight' in data:\n                    data['weight'] = data['weight'] - min_weight + 0.01\n        \n        try:\n            communities = community_louvain.best_partition(G_positive)\n            modularity = community_louvain.modularity(communities, G_positive)\n            \n            for node, community_id in communities.items():\n                louvain_results.append({\n                    'Network': name,\n                    'Community_Area': node,\n                    'Louvain_Community': community_id,\n                    'Modularity': modularity\n                })\n        except Exception as e:\n            print(f\"Warning: Louvain failed for {name}: {e}\")\n            for node in G.nodes():\n                louvain_results.append({\n                    'Network': name,\n                    'Community_Area': node,\n                    'Louvain_Community': 0,\n                    'Modularity': 0\n                })\n\nlouvain_df = pd.DataFrame(louvain_results)\nlouvain_df.to_csv('Louvain_Communities_All_Networks.csv', index=False)\n\nprint(\"Louvain community detection completed\")\nprint(f\"Results saved: Louvain_Communities_All_Networks.csv\")\nprint(f\"\nSample results:\")\nlouvain_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering\n",
    "# Using Ward linkage on the distance matrix\n",
    "\n",
    "# Convert cosine similarity to distance\n",
    "cosine_dist = 1 - cosine_sim\n",
    "np.fill_diagonal(cosine_dist, 0)  # Ensure diagonal is 0\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Convert to condensed distance matrix for linkage\n",
    "condensed_dist = squareform(cosine_dist)\n",
    "linkage_matrix = linkage(condensed_dist, method='ward')\n",
    "\n",
    "# Cut tree at different numbers of clusters\n",
    "hierarchical_results = []\n",
    "\n",
    "for n_clusters in [2, 3, 4, 5]:\n",
    "    clusters = fcluster(linkage_matrix, n_clusters, criterion='maxclust')\n",
    "    \n",
    "    for idx, node in enumerate(nodes):\n",
    "        hierarchical_results.append({\n",
    "            'Community_Area': node,\n",
    "            'N_Clusters': n_clusters,\n",
    "            'Hierarchical_Cluster': clusters[idx]\n",
    "        })\n",
    "\n",
    "hierarchical_df = pd.DataFrame(hierarchical_results)\n",
    "hierarchical_df.to_csv('Hierarchical_Clusters.csv', index=False)\n",
    "\n",
    "print(\"Hierarchical clustering completed\")\n",
    "print(f\"Results saved: Hierarchical_Clusters.csv\")\n",
    "print(f\"\\nSample results:\")\n",
    "hierarchical_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dendrogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(linkage_matrix, labels=nodes, leaf_font_size=10)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Community Area')\n",
    "plt.ylabel('Distance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Hierarchical_Clustering_Dendrogram.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Dendrogram saved: Hierarchical_Clustering_Dendrogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Louvain communities on network (selected network)\n",
    "selected_network = 'cosine_0.5'\n",
    "G = networks[selected_network]\n",
    "communities = community_louvain.best_partition(G)\n",
    "\n",
    "# Create color map\n",
    "community_colors = [communities[node] for node in G.nodes()]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "pos = nx.spring_layout(G, seed=42, k=2)\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_color=community_colors, \n",
    "                       node_size=1000, cmap='Set3', alpha=0.9)\n",
    "nx.draw_networkx_edges(G, pos, width=2, alpha=0.3)\n",
    "nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "\n",
    "plt.title(f'Louvain Communities - {selected_network}\\nModularity: {community_louvain.modularity(communities, G):.3f}',\n",
    "          fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'Louvain_Communities_{selected_network}.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Louvain community visualization saved for {selected_network}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering Coefficients (Block-Group Level Analysis)\n",
    "\n",
    "Computing clustering coefficients focusing on within-CA variation using block-group level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate clustering coefficients for each network\n",
    "clustering_results = []\n",
    "\n",
    "for name, G in networks.items():\n",
    "    if G.number_of_edges() > 0:\n",
    "        # Global clustering coefficient\n",
    "        global_clustering = nx.transitivity(G)\n",
    "        \n",
    "        # Average clustering coefficient\n",
    "        avg_clustering = nx.average_clustering(G)\n",
    "        \n",
    "        # Local clustering coefficient for each node\n",
    "        local_clustering = nx.clustering(G)\n",
    "        \n",
    "        for node, local_coeff in local_clustering.items():\n",
    "            clustering_results.append({\n",
    "                'Network': name,\n",
    "                'Community_Area': node,\n",
    "                'Local_Clustering': local_coeff,\n",
    "                'Global_Clustering': global_clustering,\n",
    "                'Average_Clustering': avg_clustering\n",
    "            })\n",
    "\n",
    "clustering_df = pd.DataFrame(clustering_results)\n",
    "clustering_df.to_csv('Clustering_Coefficients_All_Networks.csv', index=False)\n",
    "\n",
    "print(\"Clustering coefficients computed\")\n",
    "print(f\"Results saved: Clustering_Coefficients_All_Networks.csv\")\n",
    "print(f\"\\nSample results:\")\n",
    "clustering_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of clustering coefficients by network\n",
    "clustering_summary = clustering_df.groupby('Network').agg({\n",
    "    'Local_Clustering': ['mean', 'std', 'min', 'max'],\n",
    "    'Global_Clustering': 'first',\n",
    "    'Average_Clustering': 'first'\n",
    "})\n",
    "\n",
    "print(\"\\nClustering Coefficient Summary:\")\n",
    "print(clustering_summary)\n",
    "\n",
    "clustering_summary.to_csv('Clustering_Summary.csv')\n",
    "print(\"\\nSummary saved: Clustering_Summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering coefficients\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Local clustering by CA for selected network\n",
    "selected_data = clustering_df[clustering_df['Network'] == selected_network].copy()\n",
    "selected_data = selected_data.sort_values('Local_Clustering', ascending=True)\n",
    "\n",
    "axes[0].barh(selected_data['Community_Area'], selected_data['Local_Clustering'])\n",
    "axes[0].set_xlabel('Local Clustering Coefficient')\n",
    "axes[0].set_title(f'Local Clustering Coefficients\\n(Network: {selected_network})')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: Average clustering across all networks\n",
    "network_clustering = clustering_df.groupby('Network')['Average_Clustering'].first().sort_values()\n",
    "axes[1].barh(network_clustering.index, network_clustering.values)\n",
    "axes[1].set_xlabel('Average Clustering Coefficient')\n",
    "axes[1].set_title('Average Clustering by Network')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Clustering_Coefficients_Visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Clustering coefficient visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Network Graphs (GraphML Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all networks in GraphML format for external analysis\n",
    "import os\n",
    "os.makedirs('network_graphs', exist_ok=True)\n",
    "\n",
    "for name, G in networks.items():\n",
    "    # Add node attributes\n",
    "    for node in G.nodes():\n",
    "        # Add centrality as node attributes if available\n",
    "        node_centrality = centrality_df[\n",
    "            (centrality_df['Network'] == name) & \n",
    "            (centrality_df['Community_Area'] == node)\n",
    "        ]\n",
    "        \n",
    "        if not node_centrality.empty:\n",
    "            G.nodes[node]['degree_centrality'] = float(node_centrality['Degree_Centrality'].values[0])\n",
    "            G.nodes[node]['eigenvector_centrality'] = float(node_centrality['Eigenvector_Centrality'].values[0])\n",
    "            G.nodes[node]['closeness_centrality'] = float(node_centrality['Closeness_Centrality'].values[0])\n",
    "    \n",
    "    # Save as GraphML\n",
    "    nx.write_graphml(G, f'network_graphs/{name}.graphml')\n",
    "\n",
    "print(\"All networks saved in GraphML format in 'network_graphs/' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "summary_report = []\n",
    "\n",
    "for name, G in networks.items():\n",
    "    # Basic network stats\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    n_edges = G.number_of_edges()\n",
    "    density = nx.density(G)\n",
    "    \n",
    "    # Centrality stats\n",
    "    cent_data = centrality_df[centrality_df['Network'] == name]\n",
    "    avg_degree = cent_data['Degree_Centrality'].mean() if not cent_data.empty else 0\n",
    "    avg_eigen = cent_data['Eigenvector_Centrality'].mean() if not cent_data.empty else 0\n",
    "    avg_close = cent_data['Closeness_Centrality'].mean() if not cent_data.empty else 0\n",
    "    \n",
    "    # Clustering stats\n",
    "    clust_data = clustering_df[clustering_df['Network'] == name]\n",
    "    global_clust = clust_data['Global_Clustering'].iloc[0] if not clust_data.empty else 0\n",
    "    avg_clust = clust_data['Average_Clustering'].iloc[0] if not clust_data.empty else 0\n",
    "    \n",
    "    # Louvain stats\n",
    "    louv_data = louvain_df[louvain_df['Network'] == name]\n",
    "    n_communities = louv_data['Louvain_Community'].nunique() if not louv_data.empty else 0\n",
    "    modularity = louv_data['Modularity'].iloc[0] if not louv_data.empty else 0\n",
    "    \n",
    "    summary_report.append({\n",
    "        'Network': name,\n",
    "        'Nodes': n_nodes,\n",
    "        'Edges': n_edges,\n",
    "        'Density': density,\n",
    "        'Avg_Degree_Centrality': avg_degree,\n",
    "        'Avg_Eigenvector_Centrality': avg_eigen,\n",
    "        'Avg_Closeness_Centrality': avg_close,\n",
    "        'Global_Clustering': global_clust,\n",
    "        'Average_Clustering': avg_clust,\n",
    "        'N_Communities': n_communities,\n",
    "        'Modularity': modularity\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_report)\n",
    "summary_df.to_csv('Network_Summary_Report.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE NETWORK SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"\\nReport saved: Network_Summary_Report.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Deliverable ZIP File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create zip file with all outputs\n",
    "zip_filename = 'Souptik_Network_and_Metrics.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add CSV files\n",
    "    csv_files = [\n",
    "        'CA_Aggregated_Data.csv',\n",
    "        'CA_Normalized_Features.csv',\n",
    "        'Centrality_Measures_All_Networks.csv',\n",
    "        'Centrality_Summary_Statistics.csv',\n",
    "        'Louvain_Communities_All_Networks.csv',\n",
    "        'Hierarchical_Clusters.csv',\n",
    "        'Clustering_Coefficients_All_Networks.csv',\n",
    "        'Clustering_Summary.csv',\n",
    "        'Network_Summary_Report.csv'\n",
    "    ]\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        if Path(csv_file).exists():\n",
    "            zipf.write(csv_file)\n",
    "    \n",
    "    # Add visualization files\n",
    "    viz_files = [\n",
    "        'Network_Graphs_All_Versions.png',\n",
    "        f'Centrality_Visualization_{selected_network}.png',\n",
    "        f'Louvain_Communities_{selected_network}.png',\n",
    "        'Hierarchical_Clustering_Dendrogram.png',\n",
    "        'Clustering_Coefficients_Visualization.png'\n",
    "    ]\n",
    "    \n",
    "    for viz_file in viz_files:\n",
    "        if Path(viz_file).exists():\n",
    "            zipf.write(viz_file)\n",
    "    \n",
    "    # Add GraphML network files\n",
    "    if Path('network_graphs').exists():\n",
    "        for graphml_file in Path('network_graphs').glob('*.graphml'):\n",
    "            zipf.write(graphml_file)\n",
    "\n",
    "print(f\"\\n\u2705 Deliverable created: {zip_filename}\")\n",
    "print(f\"\\nContents:\")\n",
    "print(\"  - 9 CSV files with metrics and results\")\n",
    "print(\"  - 5 visualization PNG files\")\n",
    "print(\"  - 7 GraphML network graph files\")\n",
    "print(f\"\\nTotal file size: {Path(zip_filename).stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Complete!\n",
    "\n",
    "### Deliverables Summary:\n",
    "\n",
    "**1. Data Files:**\n",
    "- CA_Aggregated_Data.csv: Original aggregated data for 9 CAs\n",
    "- CA_Normalized_Features.csv: Standardized features used for network construction\n",
    "\n",
    "**2. Network Metrics:**\n",
    "- Centrality_Measures_All_Networks.csv: Degree, eigenvector, and closeness centrality\n",
    "- Clustering_Coefficients_All_Networks.csv: Local and global clustering coefficients\n",
    "- Network_Summary_Report.csv: Comprehensive summary of all networks\n",
    "\n",
    "**3. Community Detection:**\n",
    "- Louvain_Communities_All_Networks.csv: Louvain algorithm results\n",
    "- Hierarchical_Clusters.csv: Hierarchical clustering results\n",
    "\n",
    "**4. Visualizations:**\n",
    "- Network graphs for all 7 versions\n",
    "- Centrality measure comparisons\n",
    "- Community structure visualizations\n",
    "- Hierarchical clustering dendrogram\n",
    "\n",
    "**5. Network Graphs:**\n",
    "- 7 GraphML files (one for each network version) ready for import into Gephi, Cytoscape, etc.\n",
    "\n",
    "All files are packaged in: **Souptik_Network_and_Metrics.zip**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}